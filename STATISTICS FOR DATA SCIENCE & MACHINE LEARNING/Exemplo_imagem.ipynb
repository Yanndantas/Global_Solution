{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#Importando bibliotecas que serão utilizadas no código\n",
        "from numpy import zeros, ones, expand_dims, asarray\n",
        "from numpy.random import randn, randint\n",
        "from keras.datasets import fashion_mnist\n",
        "from keras.optimizers import Adam\n",
        "from keras.models import Model, load_model\n",
        "from keras.layers import Input, Dense, Reshape, Flatten\n",
        "from keras.layers import Conv2D, Conv2DTranspose, Concatenate\n",
        "from keras.layers import LeakyReLU, Dropout, Embedding\n",
        "from keras.layers import BatchNormalization, Activation\n",
        "from keras import initializers\n",
        "from keras.initializers import RandomNormal\n",
        "from keras.optimizers import Adam, RMSprop, SGD\n",
        "from matplotlib import pyplot\n",
        "import numpy as np\n",
        "from math import sqrt"
      ],
      "metadata": {
        "id": "a9n5FILW6vhP"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Carregando dataset \n",
        "# carregando apenas as características dos dados de treinamento, pois não precisamos dos rótulos.\n",
        "# Em seguida, estamos dividindo cada valor de pixel por 127,5 e subtraindo 1 para ter valores de pixel na faixa de -1 a 1. Por fim, a forma de X_train é (60000, 28, 28, 1).\n",
        "\n",
        "(X_train, _), (_, _) = fashion_mnist.load_data()\n",
        "X_train = X_train.astype(np.float32) / 127.5 - 1\n",
        "X_train = np.expand_dims(X_train, axis=3)\n",
        "print(X_train.shape)"
      ],
      "metadata": {
        "id": "S_R-m9iX6wfB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Algumas funções que são necessarias\n",
        "#  usando a função abaixo para gerar pontos latentes com a forma n_samplesxlatent_dim (100 no nosso caso).\n",
        "def generate_latent_points(latent_dim, n_samples):\n",
        "    x_input = randn(latent_dim * n_samples)  \n",
        "    z_input = x_input.reshape(n_samples, latent_dim)\n",
        "    return z_input"
      ],
      "metadata": {
        "id": "vZ9qC7fW662g"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# A função abaixo ajuda a gerar n amostras reais com o rótulo 1, ou seja, uma imagem real.\n",
        "\n",
        "def generate_real_samples(X_train, n_samples):\n",
        "    ix = randint(0, X_train.shape[0], n_samples) \n",
        "    X = X_train[ix]  \n",
        "    y = ones((n_samples, 1)) \n",
        "    return X, y"
      ],
      "metadata": {
        "id": "azx9gf0O7AIJ"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# A função acima ajuda a gerar (n) amostras falsas usando o gerador com o rótulo 0, ou seja, uma imagem falsa.\n",
        "\n",
        "def generate_fake_samples(generator, latent_dim, n_samples):\n",
        "    z_input = generate_latent_points(latent_dim, n_samples)\n",
        "    images = generator.predict(z_input)  \n",
        "    y = zeros((n_samples, 1))\n",
        "    return images, y"
      ],
      "metadata": {
        "id": "sWprVLu27Kko"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Esta função ajuda a resumir o desempenho. Isso inclui a geração de uma amostra falsa, a plotagem dela e, finalmente, o salvamento do modelo.\n",
        "\n",
        "def summarize_performance(step, g_model, latent_dim, n_samples=100):\n",
        "    X, _ = generate_fake_samples(g_model, latent_dim, n_samples)\n",
        "    X = (X + 1) / 2.0\n",
        "    for i in range(100):\n",
        "        pyplot.subplot(10, 10, 1 + i)\n",
        "        pyplot.axis('off')\n",
        "        pyplot.imshow(X[i, :, :, 0], cmap='gray_r')\n",
        "    filename2 = 'model_%04d.h5' % (step+1)\n",
        "    g_model.save(filename2)\n",
        "    print('>Saved: %s' % (filename2))"
      ],
      "metadata": {
        "id": "LiyUgAfs7Mik"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# A função abaixo ajuda a plotar os resultados. usada para plotar as imagens geradas pelo Gerador em estágios posteriores.\n",
        "\n",
        "def save_plot(examples, n_examples):\n",
        "    for i in range(n_examples):\n",
        "        pyplot.subplot(sqrt(n_examples), sqrt(n_examples), 1 + i)\n",
        "        pyplot.axis('off')\n",
        "        pyplot.imshow(examples[i, :, :, 0], cmap='gray_r')\n",
        "    pyplot.show()"
      ],
      "metadata": {
        "id": "tN9njkQ37OOf"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above function helps us to plot the results. We’ll use this to plot the generated images by the Generator in later stages.\n",
        "\n"
      ],
      "metadata": {
        "id": "FOhBUgl17ZU8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Construindo o modelo\n",
        "\n",
        "# discriminator = define_discriminator() utilizando algumas camadas Dense, Flatten e Dropout com a função de ativação leaky relu nas camadas ocultas e sigmoid na camada final.\n",
        "# Utilizando o otimizador adam e a função de perda binary cross-entropy, pois a tarefa do discriminador é realizar a classificação binária.\n",
        "\n",
        "def define_discriminator(in_shape=(28, 28, 1)):\n",
        "    init = RandomNormal(stddev=0.02)  \n",
        "    in_image = Input(shape=in_shape)\n",
        "    fe = Flatten()(in_image)\n",
        "    fe = Dense(1024)(fe)\n",
        "    fe = LeakyReLU(alpha=0.2)(fe)\n",
        "    fe = Dropout(0.3)(fe)\n",
        "    fe = Dense(512)(fe)\n",
        "    fe = LeakyReLU(alpha=0.2)(fe)\n",
        "    fe = Dropout(0.3)(fe)\n",
        "    fe = Dense(256)(fe)\n",
        "    fe = LeakyReLU(alpha=0.2)(fe)\n",
        "    fe = Dropout(0.3)(fe)\n",
        "    out = Dense(1, activation='sigmoid')(fe)\n",
        "    model = Model(in_image, out)\n",
        "    opt = Adam(lr=0.0002, beta_1=0.5) \n",
        "    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "VH6aWSkm7ZqS"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generator = define_generator(100)\n",
        "\n",
        "# utilizando algumas camadas Dense para definir o modelo do gerador, novamente com a função de ativação Leaky ReLU nas camadas ocultas e (tanh) na camada final.\n",
        "# As imagens geradas G(z) terão o formato 28x28x1.\n",
        "\n",
        "def define_generator(latent_dim): \n",
        "    init = RandomNormal(stddev=0.02)\n",
        "    in_lat = Input(shape=(latent_dim,)) \n",
        "    gen = Dense(256, kernel_initializer=init)(in_lat)\n",
        "    gen = LeakyReLU(alpha=0.2)(gen)\n",
        "    gen = Dense(512, kernel_initializer=init)(gen)\n",
        "    gen = LeakyReLU(alpha=0.2)(gen)\n",
        "    gen = Dense(1024, kernel_initializer=init)(gen)\n",
        "    gen = LeakyReLU(alpha=0.2)(gen)\n",
        "    gen = Dense(28 * 28 * 1, kernel_initializer=init)(gen)\n",
        "    out_layer = Activation('tanh')(gen)\n",
        "    out_layer = Reshape((28, 28, 1))(gen)\n",
        "    model = Model(in_lat, out_layer)\n",
        "    return model"
      ],
      "metadata": {
        "id": "oMAGJb4_7f-c"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# gan_model = define_gan(generator, discriminator) congelando o discriminador, fornecendo z como entrada e D(G(z)) como saída para o nosso modelo.\n",
        "#  utilizando o otimizador adam e a função de perda binary cross-entropy.\n",
        "\n",
        "def define_gan(g_model, d_model):\n",
        "    d_model.trainable = False\n",
        "    gan_output = d_model(g_model.output)\n",
        "    model = Model(g_model.input, gan_output)\n",
        "    opt = Adam(lr=0.0002, beta_1=0.5)\n",
        "    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "6zuGXR8o7kwT"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Treinando o modelo\n",
        "\n",
        "# Esta função nos ajuda a treinar o gerador e o discriminador. \n",
        "# Para treinar o discriminador, ela primeiro gera amostras reais, atualiza os pesos do discriminador, gera amostras falsas e depois atualiza novamente os pesos do discriminador. \n",
        "# Para treinar o gerador, ela primeiro gera pontos latentes, gera rótulos como 1 para enganar o discriminador e, em seguida, atualiza os pesos do gerador. \n",
        "# Por fim, a função resume o desempenho do modelo após algumas etapas.\n",
        "\n",
        "def train(g_model, d_model, gan_model, X_train, latent_dim, n_epochs=100, n_batch=64):\n",
        "    bat_per_epo = int(X_train.shape[0] / n_batch)\n",
        "    n_steps = bat_per_epo * n_epochs\n",
        "    for i in range(n_steps):\n",
        "        X_real, y_real = generate_real_samples(X_train, n_batch)\n",
        "        d_loss_r, d_acc_r = d_model.train_on_batch(X_real, y_real)\n",
        "        X_fake, y_fake = generate_fake_samples(g_model, latent_dim, n_batch)\n",
        "        d_loss_f, d_acc_f = d_model.train_on_batch(X_fake, y_fake)\n",
        "        z_input = generate_latent_points(latent_dim, n_batch) \n",
        "        y_gan = ones((n_batch, 1)) \n",
        "        g_loss, g_acc = gan_model.train_on_batch(z_input, y_gan)\n",
        "        print('>%d, dr[%.3f,%.3f], df[%.3f,%.3f], g[%.3f,%.3f]' % (i+1, d_loss_r,d_acc_r, d_loss_f,d_acc_f, g_loss,g_acc))\n",
        "        if (i+1) % (bat_per_epo * 1) == 0:\n",
        "            summarize_performance(i, g_model, latent_dim)"
      ],
      "metadata": {
        "id": "21-3a_CG7oHQ"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function helps us to train the generator and the discriminator. To train the Discriminator, it first generates real samples, updates the discriminator’s weights, generates fake samples, and then updates the discriminator’s weights again. To train the Generator, it first generates latent points, generates labels as 1 to fool the discriminator, and then updates the generator’s weights. Finally, the function summarizes the performance of the model after some steps."
      ],
      "metadata": {
        "id": "QBa3O5Lf7qLb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Não consegui rodar o código a partir daqui \n",
        "\n",
        "# Aqui o código está chamando a função de treinamento com 100 amostras aleatórias, 20 épocas e 64 como tamanho do lote (batch size).\n",
        "\n",
        "latent_dim = 100\n",
        "train(generator, discriminator, gan_model, X_train, latent_dim, n_epochs=20, n_batch=64)\n",
        "\n",
        "latent_dim = 100\n",
        "train(generator, discriminator, gan_model, X_train, latent_dim, n_epochs=20, n_batch=64)"
      ],
      "metadata": {
        "id": "o8WzsTJh7tZM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Aqui o código está carregando o modelo salvo mais recente, gerando pontos latentes, usando o modelo carregado para previsão e plotando os resultados.\n",
        "\n",
        "\n",
        "model = load_model('model_18740.h5')\n",
        "latent_dim = 100\n",
        "n_examples = 100\n",
        "latent_points = generate_latent_points(latent_dim, n_examples)\n",
        "X  = model.predict(latent_points)\n",
        "X = (X + 1) / 2.0\n",
        "save_plot(X, n_examples)"
      ],
      "metadata": {
        "id": "GvmwsXOU7tzn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HXXcVUwf7t2F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "g7NnfXxS7t44"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}