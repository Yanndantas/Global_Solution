{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "###Geração de texto com redes neurais "
      ],
      "metadata": {
        "id": "ULRchMV5Izs-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "g5LjqQU78PpD"
      },
      "outputs": [],
      "source": [
        "#Funções para Processamento de Texto Lendo arquivos como uma string de texto\n",
        "\n",
        "def read_file(filepath):\n",
        "    \n",
        "    with open(filepath) as f:\n",
        "        str_text = f.read()\n",
        "    \n",
        "    return str_text"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "read_file('../input/melville-moby-dick/melville-moby_dick.txt')"
      ],
      "metadata": {
        "id": "SIibfnXf8YGL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Nesse código, utilizando a biblioteca Spacy para processamento de linguagem natural em inglês.  o modelo de idioma é carregado e desativamos componentes específicos que não são necessários. \n",
        "#Também é definidi o comprimento máximo dos textos que o modelo pode processar.\n",
        "\n",
        "import spacy\n",
        "nlp = spacy.load('en',disable=['parser', 'tagger','ner'])\n",
        "\n",
        "nlp.max_length = 1198623"
      ],
      "metadata": {
        "id": "VgV73nUA8j4c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Nesse código, temos uma função chamada \"separate_punc\" que recebe um texto como entrada.\n",
        "# A função utiliza o modelo do Spacy para tokenizar o texto e retorna uma lista de tokens convertidos para letras minúsculas (lowercase), \n",
        "# excluindo os tokens que são pontuações ou caracteres especiais especificados na condição da compreensão de lista. Essa função separa e remove a pontuação do texto.\n",
        "\n",
        "def separate_punc(doc_text):\n",
        "    return [token.text.lower() for token in nlp(doc_text) if token.text not in '\\n\\n \\n\\n\\n!\"-#$%&()--.*+,-/:;<=>?@[\\\\]^_`{|}~\\t\\n ']"
      ],
      "metadata": {
        "id": "0SoAjSyb8v6i"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# \n",
        "# Nesse código, estamos lendo um arquivo chamado 'melville-moby_dick.txt' usando a função 'read_file', que provavelmente é uma função personalizada definida anteriormente no código.\n",
        "# Em seguida, estamos chamando a função 'separate_punc' passando o conteúdo do arquivo como argumento e armazenando o resultado na variável 'tokens'. \n",
        "# Isso nos dará uma lista de tokens do texto, com a pontuação separada e removida.\n",
        "\n",
        "d = read_file('../input/melville-moby-dick/melville-moby_dick.txt')\n",
        "tokens = separate_punc(d)"
      ],
      "metadata": {
        "id": "f-zlBSFX8zgf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# A variável 'tokens' conterá uma lista de tokens do texto após a remoção e separação da pontuação. Cada elemento da lista representará uma palavra ou um símbolo não pontuado encontrado no texto.\n",
        "# Esses tokens podem ser usados para análises ou processamento adicional do texto.\n",
        "\n",
        "tokens"
      ],
      "metadata": {
        "id": "0wdMl-24FTO0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# A função len(tokens) retorna o tamanho da lista de tokens, ou seja, o número de elementos na lista. \n",
        "# Isso indica a quantidade de palavras e símbolos não pontuados presentes no texto após a separação e remoção da pontuação.\n",
        "\n",
        "len(tokens)"
      ],
      "metadata": {
        "id": "POnB4cMMFTmY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# A variável max é definida como o resultado da multiplicação entre o tamanho da lista de tokens, len(tokens), e 0,75. Isso significa que max receberá 75% do tamanho total da lista de tokens. \n",
        "# Essa variável provavelmente será usada para determinar um limite máximo em algum contexto específico do código.\n",
        "\n",
        "max=int(len(tokens)*.75)\n",
        "max"
      ],
      "metadata": {
        "id": "qNuRksOEFTgs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# A variável tokens é reatribuída com uma nova lista contendo apenas os primeiros max elementos da lista original de tokens. \n",
        "# Isso significa que estamos limitando a lista de tokens ao tamanho especificado por max, descartando os elementos após a posição max.\n",
        "\n",
        "tokens=tokens[:max]"
      ],
      "metadata": {
        "id": "2cdYqKTnFWlL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Criando uma sequencia de tokens \n"
      ],
      "metadata": {
        "id": "_8igpISMFYID"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# O comprimento de treinamento train_len é definido como 25 + 1, o que significa que teremos 25 palavras de treinamento e depois uma palavra alvo.\n",
        "\n",
        "# A variável text_sequences é inicializada como uma lista vazia. Em seguida, é realizado um loop começando do valor train_len até o tamanho total da lista de tokens (len(tokens)).\n",
        "# A cada iteração do loop, é criada uma sequência seq de palavras que contém as 25 palavras anteriores à palavra atual. Essa sequência é então adicionada à lista text_sequences.\n",
        "\n",
        "# Esse processo é repetido até que todas as sequências possíveis de tamanho train_len sejam criadas a partir dos tokens disponíveis. \n",
        "# O objetivo é preparar os dados para treinar um modelo de rede neural que será capaz de prever a próxima palavra com base nas palavras anteriores.\n",
        "\n",
        "\n",
        "train_len = 25+1 \n",
        "\n",
        "text_sequences = []\n",
        "\n",
        "for i in range(train_len, len(tokens)):\n",
        "    \n",
        "    \n",
        "    seq = tokens[i-train_len:i]\n",
        "    \n",
        "    \n",
        "    text_sequences.append(seq)"
      ],
      "metadata": {
        "id": "6FDKcmvuHfKT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# O código ' '.join(text_sequences[0]) junta as palavras da primeira sequência de texto em uma única string, separando cada palavra por um espaço. Por exemplo, \n",
        "# se a primeira sequência for ['palavra1', 'palavra2', 'palavra3'], a saída será \"palavra1 palavra2 palavra3\".\n",
        "\n",
        "\n",
        "# De forma semelhante, ' '.join(text_sequences[1]) junta as palavras da segunda sequência de texto em uma única string.\n",
        "\n",
        "\n",
        "# ' '.join(text_sequences[2]) junta as palavras da terceira sequência de texto em uma única string.\n",
        "\n",
        "\n",
        "# len(text_sequences) retorna o tamanho da lista text_sequences, ou seja, o número total de sequências de texto criadas.\n",
        "\n",
        "\n",
        "# O código max = int(len(text_sequences) * .75) calcula o valor máximo com base no comprimento das sequências de texto. \n",
        "# Ele multiplica o comprimento total das sequências de texto por 0,75 e converte o resultado para um número inteiro usando a função int(). O valor resultante é atribuído à variável max.\n",
        "\n",
        "\n",
        "# A linha de código text_sequences = text_sequences[:max] seleciona um subconjunto das sequências de texto. \n",
        "# Ele usa a variável max para determinar a quantidade de elementos que serão mantidos na lista text_sequences.\n",
        "#  Através dessa operação, text_sequences é atualizado para conter apenas os primeiros max elementos da lista original."
      ],
      "metadata": {
        "id": "LStmPp2GIK0z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "' '.join(text_sequences[0])"
      ],
      "metadata": {
        "id": "Yqjwk4A9HfTF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "' '.join(text_sequences[1])"
      ],
      "metadata": {
        "id": "mpJr2axUH0zR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "' '.join(text_sequences[2])"
      ],
      "metadata": {
        "id": "eE16cu6NH3TS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(text_sequences)"
      ],
      "metadata": {
        "id": "mvoEnqdgH43I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max=int(len(text_sequences)*.75)\n",
        "max"
      ],
      "metadata": {
        "id": "IC4kHgiiIPA6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_sequences=text_sequences[:max]"
      ],
      "metadata": {
        "id": "e1-ic8cvIPac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YENNlYUVIhk6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Keras"
      ],
      "metadata": {
        "id": "QWJoey2uI6I4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# A função Tokenizer do Keras é usada para dividir o texto em tokens e criar um dicionário que associa cada token a um índice único.\n",
        "# Isso é útil para o pré-processamento de dados de texto antes de alimentá-los em modelos de redes neurais.\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer"
      ],
      "metadata": {
        "id": "un91b2xuI7-v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#O código  cria um objeto Tokenizer e o ajusta aos textos fornecidos. Em seguida, os textos são convertidos em sequências de números inteiros usando o método texts_to_sequences do objeto Tokenizer.\n",
        "# Essas sequências de números inteiros podem ser usadas como entrada para modelos de redes neurais.\n",
        "\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(text_sequences)\n",
        "sequences = tokenizer.texts_to_sequences(text_sequences)"
      ],
      "metadata": {
        "id": "icpecxBxI-2T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dBi_n0iiJ0FI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sequences[0] retorna a primeira sequência numérica de palavras após a tokenização. A saída será uma lista de números inteiros representando as palavras na primeira sequência.\n",
        "\n",
        "\n",
        "# tokenizer.index_word retorna um dicionário em que as chaves são os índices das palavras e os valores são as palavras correspondentes.\n",
        "#  Ele mapeia os índices das palavras para as palavras originais do corpus de texto.\n",
        "\n",
        "\n",
        "# Para a sequência sequences[0], o código itera sobre cada índice i e imprime o índice e a palavra correspondente usando o tokenizer.index_word.\n",
        "# Ele imprime o índice e a palavra associada para cada elemento da sequência.\n",
        "\n",
        "\n",
        "# tokenizer.word_counts é um dicionário que contém a contagem de ocorrências de cada palavra no corpus de texto. As chaves do dicionário são as palavras e os valores são as contagens correspondentes.\n",
        "# Ele fornece uma visão geral das frequências das palavras no texto analisado.\n",
        "\n",
        "\n",
        "# A variável vocabulary_size armazena o tamanho do vocabulário, ou seja, o número total de palavras distintas no corpus de texto. \n",
        "# É calculado utilizando o número de elementos no dicionário tokenizer.word_counts.\n"
      ],
      "metadata": {
        "id": "ZH_6l8noJzt5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequences[0]"
      ],
      "metadata": {
        "id": "kyK8IhEOJpqM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.index_word"
      ],
      "metadata": {
        "id": "G3T-8OJ4JqmP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in sequences[0]:\n",
        "    print(f'{i} : {tokenizer.index_word[i]}')"
      ],
      "metadata": {
        "id": "QmLmdx2GJqj8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.word_counts"
      ],
      "metadata": {
        "id": "I_qoL1gzJqhw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocabulary_size = len(tokenizer.word_counts)"
      ],
      "metadata": {
        "id": "rtI202-5Juzn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vn-xlFrrJu6G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convertendo pra a matriz numpy"
      ],
      "metadata": {
        "id": "MGDpBxkmKcfj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# importando a biblioteca NumPy\n",
        "\n",
        "# A linha de código \"sequences = np.array(sequences)\" está convertendo a lista \"sequences\" em uma matriz NumPy. Isso significa que cada elemento da lista se tornará um elemento da matriz NumPy\n",
        "\n",
        "# A variável \"sequences\" contém a matriz NumPy resultante da conversão da lista de sequências."
      ],
      "metadata": {
        "id": "xEBV_IvhNZMX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "3e5HiDwTMcSE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = np.array(sequences)"
      ],
      "metadata": {
        "id": "Bpxt8wVQMnRq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequences"
      ],
      "metadata": {
        "id": "bMdkzWLRMocf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Criando um modelo baseado em LSTM\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NdKwT9AFN1Dv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Neste trecho de código, estamos importando as bibliotecas e classes necessárias do Keras para criar um modelo LSTM. \n",
        "# Isso inclui a classe Sequential para criar modelos sequenciais, as camadas Dense, LSTM, Embedding e Dropout para construir o modelo LSTM.\n",
        "# Essas classes fornecem as funcionalidades necessárias para criar redes neurais LSTM e aplicá-las a tarefas de geração de texto.\n",
        "\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,LSTM,Embedding,Dropout"
      ],
      "metadata": {
        "id": "hXK1pjeyN2i7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# A função create_model é responsável por criar e configurar um modelo de rede neural LSTM utilizando a biblioteca Keras. \n",
        "# O modelo consiste em camadas de Embedding, LSTM e Dense, seguidas por uma camada de saída.\n",
        "\n",
        "# A camada de Embedding é usada para representar as palavras como vetores densos. Em seguida, temos duas camadas LSTM para processar as sequências de entrada e capturar informações contextuais.\n",
        "# A camada Dense é adicionada para reduzir a dimensionalidade da saída da camada LSTM.\n",
        "\n",
        "# A camada de saída usa a função de ativação softmax para gerar as probabilidades de ocorrência das palavras no vocabulário. \n",
        "# O modelo é compilado com a função de perda categorical_crossentropy, o otimizador Adam e a métrica de acurácia.\n",
        "\n",
        "# Ao final, um resumo do modelo é exibido, mostrando a arquitetura das camadas e o número de parâmetros. O modelo é retornado para ser utilizado posteriormente."
      ],
      "metadata": {
        "id": "M9EUOL37O6dD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "def create_model(vocabulary_size, seq_len):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(vocabulary_size, 25, input_length=seq_len))\n",
        "    model.add(LSTM(150, return_sequences=True))\n",
        "    model.add(LSTM(150))\n",
        "    model.add(Dense(150, activation='relu'))\n",
        "\n",
        "    model.add(Dense(vocabulary_size, activation='softmax'))\n",
        "    \n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "   \n",
        "    model.summary()\n",
        "    \n",
        "    return model"
      ],
      "metadata": {
        "id": "Tgay3fXsOYA3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train / Test Split"
      ],
      "metadata": {
        "id": "YwnWfcjwPCVC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Em resumo, esses comandos estão preparando os dados para o treinamento do modelo, dividindo as sequências em características de entrada (X) e rótulos de saída (y), \n",
        "#convertendo os rótulos em uma representação adequada para classificação e obtendo o comprimento da sequência para definição de parâmetros do modelo.\n"
      ],
      "metadata": {
        "id": "9iNu_SsCPoi8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.utils import to_categorical"
      ],
      "metadata": {
        "id": "BM2DKsBhPdDM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequences"
      ],
      "metadata": {
        "id": "g7PJqaZqPdI0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequences[:,:-1]"
      ],
      "metadata": {
        "id": "-NfwG_TJPfYu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequences[:,-1]"
      ],
      "metadata": {
        "id": "0yV0FI-XPfW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = sequences[:,:-1]"
      ],
      "metadata": {
        "id": "PTNz9r95PfN7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = sequences[:,-1]"
      ],
      "metadata": {
        "id": "Eo8hn-qqPfLv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = to_categorical(y, num_classes=vocabulary_size+1)"
      ],
      "metadata": {
        "id": "3bNdj2n8PjFk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq_len = X.shape[1]"
      ],
      "metadata": {
        "id": "I6KhXrIVPjDn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq_len"
      ],
      "metadata": {
        "id": "f51DIHRPPjBV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pIAfN7yfQPuv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Treinando o modelo"
      ],
      "metadata": {
        "id": "_87pWbc-QSIe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Esses comandos criam, treinam e salvam um modelo LSTM com base nos dados fornecidos, além de salvar o objeto \"tokenizer\" para uso posterior."
      ],
      "metadata": {
        "id": "1_jldyDjQ3sL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = create_model(vocabulary_size+1, seq_len)"
      ],
      "metadata": {
        "id": "AknOF0vLQTVY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pickle import dump,load"
      ],
      "metadata": {
        "id": "QAbtvntUQV1d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X, y, batch_size=512, epochs=300,verbose=1)"
      ],
      "metadata": {
        "id": "jtkz6ae1QYSP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('./epochBIG.h5')\n",
        "dump(tokenizer, open('./epochBIG', 'wb'))"
      ],
      "metadata": {
        "id": "3oxH1_7uQYQB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qPsRQghWQ8TJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Gerando um novo texto"
      ],
      "metadata": {
        "id": "5FxzLMgAQ8vK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Esses comandos importam as funções necessárias para gerar números aleatórios, carregar objetos serializados,\n",
        "# carregar um modelo treinado e realizar o preenchimento de sequências com base em um comprimento máximo.\n",
        "\n",
        "from random import randint\n",
        "from pickle import load\n",
        "from keras.models import load_model\n",
        "from keras.preprocessing.sequence import pad_sequences"
      ],
      "metadata": {
        "id": "2OSHvYFTQ_b1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Essa função gera texto com base no modelo treinado, usando uma Seed inicial e predizendo palavras subsequentes a partir do modelo.\n",
        "\n",
        "\n",
        "def generate_text(model, tokenizer, seq_len, seed_text, num_gen_words):\n",
        "    '''\n",
        "    INPUTS:\n",
        "    model : model that was trained on text data\n",
        "    tokenizer : tokenizer that was fit on text data\n",
        "    seq_len : length of training sequence\n",
        "    seed_text : raw string text to serve as the seed\n",
        "    num_gen_words : number of words to be generated by model\n",
        "    '''\n",
        "    \n",
        "    # Final Output\n",
        "    output_text = []\n",
        "    \n",
        "    # Intial Seed Sequence\n",
        "    input_text = seed_text\n",
        "    \n",
        "    # Create num_gen_words\n",
        "    for i in range(num_gen_words):\n",
        "        \n",
        "        # Take the input text string and encode it to a sequence\n",
        "        encoded_text = tokenizer.texts_to_sequences([input_text])[0]\n",
        "        \n",
        "        # Pad sequences to our trained rate (50 words in the video)\n",
        "        pad_encoded = pad_sequences([encoded_text], maxlen=seq_len, truncating='pre')\n",
        "        \n",
        "        # Predict Class Probabilities for each word\n",
        "        pred_word_ind = model.predict_classes(pad_encoded, verbose=0)[0]\n",
        "        \n",
        "        # Grab word\n",
        "        pred_word = tokenizer.index_word[pred_word_ind] \n",
        "        \n",
        "        # Update the sequence of input text (shifting one over with the new word)\n",
        "        input_text += ' ' + pred_word\n",
        "        \n",
        "        output_text.append(pred_word)\n",
        "        \n",
        "    # Make it look like a sentence.\n",
        "    return ' '.join(output_text)"
      ],
      "metadata": {
        "id": "0DY-5UFiRT-f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Escolhendo uma sequencia de seed aleatória"
      ],
      "metadata": {
        "id": "7VBOIXtZRT8w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# esses comandos envolvem a seleção aleatória de uma sequência de texto, \n",
        "# a formatação dessa sequência como uma semente de texto e a geração de texto adicional com base nessa semente usando o modelo treinado."
      ],
      "metadata": {
        "id": "jVvdmshKUZmO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_sequences[0]"
      ],
      "metadata": {
        "id": "WMUSaLeaRT6J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "random.seed(101)\n",
        "random_pick = random.randint(0,len(text_sequences))"
      ],
      "metadata": {
        "id": "VoYqQWmfRT3x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random_seed_text = text_sequences[random_pick]"
      ],
      "metadata": {
        "id": "aQwlhRXsRT0q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random_seed_text"
      ],
      "metadata": {
        "id": "x8cV6-RbUUiO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed_text = ' '.join(random_seed_text)"
      ],
      "metadata": {
        "id": "W5gfsBEzUVu4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed_text"
      ],
      "metadata": {
        "id": "3tB4BNHZUWp9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_text(model,tokenizer,seq_len,seed_text=seed_text,num_gen_words=50)"
      ],
      "metadata": {
        "id": "gTP8xZZsUWnn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ywOSnPl_UWlp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Explorando a Sequência Gerada\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DwE6TScoVOqt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Esses comandos permitem explorar o contexto em que a palavra \"inkling\" aparece no texto completo, \n",
        "# exibindo as palavras antes e depois dela. Isso pode ajudar a entender melhor o contexto e o significado da palavra dentro do texto"
      ],
      "metadata": {
        "id": "YDTclHXYVtbn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_text = read_file('../input/melville-moby-dick/melville-moby_dick.txt')"
      ],
      "metadata": {
        "id": "bMlFGoTmVPZ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i,word in enumerate(full_text.split()):\n",
        "    if word == 'inkling':\n",
        "        print(' '.join(full_text.split()[i-20:i+20]))\n",
        "        print('\\n')"
      ],
      "metadata": {
        "id": "SCmJMZtEVSPC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}